# run_ingestion.py

import sys
import asyncio
from pathlib import Path
from dotenv import load_dotenv

# Add project root to sys.path
workspace_root = Path(__file__).resolve().parent
if str(workspace_root) not in sys.path:
    sys.path.insert(0, str(workspace_root))

from scrape.foodScrapper import RecipeScraper
from scrape.prepareEmbeddings import RecipeEmbeddingPrep
from scrape.generateEmbeddings import RecipeEmbedder

load_dotenv()


async def scrape_recipes_pipeline():
    """
    Scrape recipes from web and save to backup JSON and DB.
    """
    scraper = RecipeScraper(
        allowed_domains=[
            "www.food.com",
            "www.allrecipes.com",
            "www.seriouseats.com",
            "www.bbcgoodfood.com",
            "www.blueapron.com",
            "www.foodnetwork.com",
            "www.tasteofhome.com",
            "www.loveandlemons.com",
            "www.simplyrecipes.com",
            "www.delish.com",
            "www.bonappetit.com",
            "www.epicurious.com",
            "www.cookieandkate.com",
            "www.budgetbytes.com",
        ],
        max_recipes=1500,  # Increased from 300 to 1500 for daily scraping
        db_path="recipes.db",
    )

    # More diverse start URLs to find more recipes
    start_urls = [
        # Food.com
        "https://www.food.com/ideas",
        "https://www.food.com/recipes",

        # AllRecipes
        "https://www.allrecipes.com/recipes/",
        "https://www.allrecipes.com/recipes/17562/dinner/",
        "https://www.allrecipes.com/recipes/78/breakfast-and-brunch/",

        # Serious Eats
        "https://www.seriouseats.com/recipes",
        "https://www.seriouseats.com/easy-recipes-5117887",

        # BBC Good Food
        "https://www.bbcgoodfood.com/recipes/collection/quick-recipes",
        "https://www.bbcgoodfood.com/recipes/collection/easy-recipes",
        "https://www.bbcgoodfood.com/recipes",

        # Blue Apron
        "https://www.blueapron.com/cookbook/",

        # Food Network
        "https://www.foodnetwork.com/recipes",
        "https://www.foodnetwork.com/recipes/photos/favorite-vegetarian-recipes",

        # Taste of Home
        "https://www.tasteofhome.com/recipes/",
        "https://www.tasteofhome.com/collection/vegetarian-dinners/",

        # Love and Lemons
        "https://www.loveandlemons.com/recipes/",
        "https://www.loveandlemons.com/vegetarian-recipes/",

        # Simply Recipes
        "https://www.simplyrecipes.com/recipes/",
        "https://www.simplyrecipes.com/dinner_recipes/",

        # Delish
        "https://www.delish.com/cooking/recipe-ideas/",
        "https://www.delish.com/cooking/g3273/easy-dinner-recipes/",

        # Bon Appetit
        "https://www.bonappetit.com/recipes",
        "https://www.bonappetit.com/recipes/quick-easy",

        # Epicurious
        "https://www.epicurious.com/recipes-menus",
        "https://www.epicurious.com/recipes/food/views/quick-easy",

        # Cookie and Kate
        "https://cookieandkate.com/recipes/",
        "https://cookieandkate.com/category/recipes/vegetarian/",

        # Budget Bytes
        "https://www.budgetbytes.com/category/recipes/",
        "https://www.budgetbytes.com/category/recipes/dinner/",
    ]

    print(f"üöÄ Scraping recipes from {len(start_urls)} start URLs...")
    print(f"üéØ Target: {scraper.max_recipes} recipes\n")
    
    recipes = await scraper.scrape_recipes(start_urls)
    scraper.export_to_json("recipes_backup.json")
    
    print(f"\n‚úÖ Final count: {len(recipes)} recipes scraped")
    return recipes


def prepare_recipes_for_embedding():
    """
    Load recipes from DB and export to JSONL for embeddings.
    """
    print("\nüì¶ Preparing recipes for embeddings...")
    prep = RecipeEmbeddingPrep(db_path="recipes.db")
    docs = prep.load_recipes_from_db()

    if not docs:
        print("‚ùå No recipes found in DB. Aborting.")
        return None

    prep.get_statistics()
    jsonl_path = "recipes_for_embedding.jsonl"
    prep.export_for_embedding(jsonl_path)
    print(f"‚úÖ Prepared {len(docs)} recipes for embedding ‚Üí {jsonl_path}")
    return jsonl_path


def generate_and_ingest_embeddings(jsonl_path):
    """
    Generate embeddings and ingest into ChromaDB.
    CRITICAL: Use the SAME path that main.py uses!
    """
    print("\n‚ö° Generating embeddings and ingesting into ChromaDB...")

    # FIXED: Correct path calculation
    # If run_ingestion.py is in /backend/, then:
    # Path(__file__).resolve().parent = /backend/
    # Path(__file__).resolve().parent / "chroma_db" = /backend/chroma_db ‚úÖ
    chroma_dir = Path(__file__).resolve().parent / "chroma_db"
    chroma_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"üìÅ ChromaDB directory: {chroma_dir}")
    print(f"üìÅ Absolute path: {chroma_dir.absolute()}")

    embedder = RecipeEmbedder(
        provider="gemini",
        model="models/text-embedding-004",
        persist_dir=str(chroma_dir),
        collection_name="recipes",
        batch_size=32,
        embedding_dim=768,
        chunk_size=300
    )

    embedder.ingest_jsonl(jsonl_path)
    print(f"‚úÖ Embeddings ingested into ChromaDB at: {chroma_dir}")
    print(f"   Collection: '{embedder.collection_name}'")


async def main():
    import time
    
    start_time = time.time()
    
    print("\nüéØ Starting full ingestion pipeline...\n")
    print(f"üìÇ Script location: {Path(__file__).resolve()}")
    print(f"üìÇ Working directory: {Path.cwd()}")
    print(f"üìÇ Target ChromaDB: {Path(__file__).resolve().parent / 'chroma_db'}")
    print()
    
    # 1Ô∏è‚É£ Scrape recipes
    scrape_start = time.time()
    await scrape_recipes_pipeline()
    scrape_time = time.time() - scrape_start
    print(f"\n‚è±Ô∏è  Scraping took: {scrape_time:.2f} seconds ({scrape_time/60:.2f} minutes)")
    
    # 2Ô∏è‚É£ Prepare recipes for embeddings
    prep_start = time.time()
    jsonl_path = prepare_recipes_for_embedding()
    if not jsonl_path:
        return
    prep_time = time.time() - prep_start
    print(f"\n‚è±Ô∏è  Preparation took: {prep_time:.2f} seconds")

    # 3Ô∏è‚É£ Generate embeddings and ingest to Chroma
    embed_start = time.time()
    generate_and_ingest_embeddings(jsonl_path)
    embed_time = time.time() - embed_start
    print(f"\n‚è±Ô∏è  Embedding generation took: {embed_time:.2f} seconds ({embed_time/60:.2f} minutes)")
    
    total_time = time.time() - start_time

    print("\nüéâ Full ingestion pipeline completed successfully!")
    print(f"\n‚è±Ô∏è  TOTAL TIME: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
    print(f"   - Scraping: {scrape_time/60:.2f} min")
    print(f"   - Preparation: {prep_time:.2f} sec")
    print(f"   - Embeddings: {embed_time/60:.2f} min")
    print("\nüìã Next steps:")
    print("   1. Run: python debug_chroma_db.py")
    print("   2. Verify recipes are loaded")
    print("   3. Start your server: python backend/main.py")


if __name__ == "__main__":
    asyncio.run(main())