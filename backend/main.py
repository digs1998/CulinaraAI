# backend/main.py

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict
from contextlib import asynccontextmanager
from rag_engine import RecipeRAGEngine
import uvicorn
import sys
from pathlib import Path
import os

# Initialize RAG engine on startup
rag_engine = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    global rag_engine
    print("ðŸš€ Initializing Feastron RAG Engine...")
    # Get the path relative to this file's location
    backend_dir = os.path.dirname(os.path.abspath(__file__))
    embeddings_path = os.path.join(backend_dir, "data", "recipe_embeddings.npz")
    rag_engine = RecipeRAGEngine(embeddings_path=embeddings_path)
    rag_engine.setup_mcp_orchestrator()  # Setup MCP orchestrator
    print("âœ… RAG Engine + MCP Orchestrator ready!")
    yield
    # Shutdown (if needed, cleanup code goes here)
    # For now, no cleanup needed

app = FastAPI(title="Feastron API", version="1.0.0", lifespan=lifespan)

# Enable CORS for frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:5173"],  # Vite/React default ports
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============= Request/Response Models =============

class ChatMessage(BaseModel):
    message: str
    conversation_id: Optional[str] = None


class RecipeSearchRequest(BaseModel):
    query: str
    top_k: Optional[int] = 5
    filters: Optional[Dict] = None
    min_score: Optional[float] = 0.0


class RecipeResponse(BaseModel):
    id: str
    title: str
    category: Optional[str]
    rating: Optional[float]
    total_time: Optional[str]
    url: str
    score: float


class ChatResponse(BaseModel):
    message: str
    recipes: List[RecipeResponse]
    sources: List[Dict]


# ============= API Endpoints =============

@app.get("/")
async def root():
    """Health check"""
    return {
        "status": "healthy",
        "service": "Feastron API",
        "version": "1.0.0",
        "endpoints": {
            "docs": "/docs",
            "chat": "POST /api/chat",
            "search": "POST /api/search",
            "stats": "GET /api/stats"
        }
    }


@app.get("/api/stats")
async def get_stats():
    """Get recipe database statistics"""
    try:
        stats = rag_engine.get_statistics()
        return stats
    except Exception as e:
        import traceback
        error_detail = str(e)
        print(f"Error in /api/chat: {error_detail}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=error_detail)


@app.post("/api/search")
async def search_recipes(request: RecipeSearchRequest):
    """Search recipes using semantic similarity"""
    try:
        results = rag_engine.search_recipes(
            query=request.query,
            top_k=request.top_k,
            filters=request.filters,
            min_score=request.min_score
        )
        
        # Format results
        recipes = []
        for result in results:
            meta = result['metadata']
            recipes.append({
                "id": result['id'],
                "title": meta['title'],
                "category": meta.get('category'),
                "rating": meta.get('rating'),
                "total_time": meta.get('total_time'),
                "url": meta.get('url'),
                "score": result['score']
            })
        
        return {"results": recipes}
    
    except Exception as e:
        import traceback
        error_detail = str(e)
        print(f"Error in /api/chat: {error_detail}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=error_detail)


@app.post("/api/chat", response_model=ChatResponse)
async def chat(request: ChatMessage):
    """
    Main chat endpoint - Uses RAG with LLM fallback
    
    Takes a user message and returns:
    - Recipe recommendations if found in database
    - LLM-generated guidance if no relevant recipes found
    """
    try:
        # Use RAG engine with similarity threshold
        # Increased threshold to 0.65 (65%) to avoid irrelevant matches
        answer = rag_engine.answer_question(
            question=request.message,
            top_k=3,
            similarity_threshold=0.65  # Require at least 65% match for better relevance
        )
        
        # Check if response was generated by LLM (no relevant recipes found)
        if answer.get('generated', False):
            # LLM generated response
            return {
                "message": f"ðŸ¤” {answer['response']}\n\n{answer.get('message', '')}",
                "recipes": [],  # No exact matches
                "sources": answer.get('sources', [])  # Might have similar recipes
            }
        
        # We have relevant recipes from database
        results = answer['sources']
        
        if not results:
            return {
                "message": "I couldn't find any recipes matching your request. Try asking for something more specific or different!",
                "recipes": [],
                "sources": []
            }
        
        # Format recipe recommendations
        recipes = []
        for result in results:
            meta = result['metadata']
            recipes.append({
                "id": result['id'],
                "title": meta['title'],
                "category": meta.get('category'),
                "rating": meta.get('rating'),
                "total_time": meta.get('total_time'),
                "url": meta.get('url'),
                "score": result['score']
            })
        
        # Generate AI response message for database results
        top_recipe = results[0]['metadata']
        response_message = f"I found some great options for you! The top match is **{top_recipe['title']}** "
        
        if top_recipe.get('category'):
            response_message += f"from the {top_recipe['category']} category "
        
        if top_recipe.get('total_time'):
            response_message += f"(takes {top_recipe['total_time']}) "
        
        if top_recipe.get('rating'):
            response_message += f"with a {top_recipe['rating']}/5 rating"
        
        response_message += f". Check out the recipes below!"
        
        return {
            "message": response_message,
            "recipes": recipes,
            "sources": results
        }
    
    except Exception as e:
        import traceback
        error_detail = str(e)
        print(f"Error in /api/chat: {error_detail}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=error_detail)


@app.get("/api/recipe/{recipe_id}")
async def get_recipe(recipe_id: str):
    """Get full recipe details"""
    try:
        details = rag_engine.get_recipe_details(recipe_id)
        if not details:
            raise HTTPException(status_code=404, detail="Recipe not found")
        return details
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_detail = str(e)
        print(f"Error in /api/chat: {error_detail}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=error_detail)


@app.get("/api/similar/{recipe_id}")
async def get_similar_recipes(recipe_id: str, top_k: int = 5):
    """Get similar recipes"""
    try:
        similar = rag_engine.get_similar_recipes(recipe_id, top_k=top_k)
        return {"similar_recipes": similar}
    except Exception as e:
        import traceback
        error_detail = str(e)
        print(f"Error in /api/chat: {error_detail}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=error_detail)


if __name__ == "__main__":
    # Add workspace root to Python path so backend.main can be imported
    workspace_root = Path(__file__).parent.parent
    if str(workspace_root) not in sys.path:
        sys.path.insert(0, str(workspace_root))
    
    # Use import string format for reload to work properly
    uvicorn.run("backend.main:app", host="0.0.0.0", port=8000, reload=True)